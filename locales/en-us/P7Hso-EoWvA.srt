1
00:00:00,420 --> 00:00:02,950
Is it ever a good idea to
ramp up your experiment?

2
00:00:02,950 --> 00:00:05,870
That is, maybe you start with
1% of your traffic as being

3
00:00:05,870 --> 00:00:07,400
diverted to the experiment group, but

4
00:00:07,400 --> 00:00:10,880
then you gradually increase that
until your feature is fully launched.

5
00:00:10,880 --> 00:00:13,570
>> Actually, I think it's good
practice to always do a ramp-up

6
00:00:13,570 --> 00:00:15,320
when you actually
want to launch a change.

7
00:00:15,320 --> 00:00:17,900
In fact, we do that for
all of our launches at Google.

8
00:00:17,900 --> 00:00:20,690
The other thing that we do is that
we remove all of the filters.

9
00:00:20,690 --> 00:00:23,579
So if you're only testing your
change on English, for example,

10
00:00:23,579 --> 00:00:26,550
you want to test your change
during your ramp-up on all users.

11
00:00:26,550 --> 00:00:31,040
Because what you want to know is if
there's been any incidental impact to

12
00:00:31,040 --> 00:00:35,050
unaffected users that you didn't
test in the original experiment.

13
00:00:35,050 --> 00:00:37,320
Now, there's one
interesting complication or

14
00:00:37,320 --> 00:00:40,250
gotcha that we do have to deal
with in our ramp-up wherein so

15
00:00:40,250 --> 00:00:44,310
that the effect may actually flatten
out as you ramp up the change.

16
00:00:44,310 --> 00:00:47,360
>> Wait, even if the initial
effect was statically significant?

17
00:00:47,360 --> 00:00:50,170
I thought the point of statistical
significance is that your results will

18
00:00:50,170 --> 00:00:51,133
be repeatable.

19
00:00:51,133 --> 00:00:55,101
>> Well, there can be some really simple
reasons as to why the effects are not

20
00:00:55,101 --> 00:00:55,870
repeatable.

21
00:00:55,870 --> 00:00:59,320
For example, there's all
sorts of seasonality effects.

22
00:00:59,320 --> 00:01:00,970
Students, for example.

23
00:01:00,970 --> 00:01:03,040
When students go on summer vacation,

24
00:01:03,040 --> 00:01:06,100
the behavior across broad
swaths of the Internet changes.

25
00:01:06,100 --> 00:01:08,860
And when they come back from vacation,
it changes again.

26
00:01:08,860 --> 00:01:10,910
There might be other seasonal
effects like holidays.

27
00:01:10,910 --> 00:01:13,430
Everyone's heard of Black Friday and
Cyber Monday.

28
00:01:13,430 --> 00:01:17,080
So shopping behavior can change
a lot around the holidays.

29
00:01:17,080 --> 00:01:20,085
Now, one way to try and
capture these seasonal or

30
00:01:20,085 --> 00:01:23,310
event-driven impacts is something
that we call a holdback.

31
00:01:23,310 --> 00:01:25,940
And the idea is that you launch
your change to everyone except for

32
00:01:25,940 --> 00:01:30,320
a small holdback, or, basically, a set
of users, that don't get the change, and

33
00:01:30,320 --> 00:01:33,390
you continue comparing their
behavior to the control.

34
00:01:33,390 --> 00:01:36,840
Now, in that case you
should see the reverse of

35
00:01:36,840 --> 00:01:39,080
the impact that you saw
in your experiment.

36
00:01:39,080 --> 00:01:42,390
And what you can do is you can track
that over time until you're confident

37
00:01:42,390 --> 00:01:43,950
that your results
are actually repeatable.

38
00:01:43,950 --> 00:01:46,864
And that can really help capture
a lot of your seasonal or

39
00:01:46,864 --> 00:01:48,229
event-driven impacts.

40
00:01:48,229 --> 00:01:52,370
>> Okay, is there anything else that can
cause this disappearing launch effect?

41
00:01:52,370 --> 00:01:55,870
>> Well, we've talked about this
a couple times in the other lessons but

42
00:01:55,870 --> 00:01:58,378
there can be a novelty effect or
change aversion.

43
00:01:58,378 --> 00:01:59,999
Those humans they're so troublesome.

44
00:01:59,999 --> 00:02:02,840
They keep changing their behavior,
right?

45
00:02:02,840 --> 00:02:06,710
But as users discover or adopt or

46
00:02:06,710 --> 00:02:11,770
change their adoption of your change,
then their behavior can change and

47
00:02:11,770 --> 00:02:13,180
therefore the measured
effects can change.

48
00:02:13,180 --> 00:02:16,810
And this is where a cohort
analysis can really be helpful.

49
00:02:16,810 --> 00:02:21,110
Now, another situation that may also
happen is that if you have advertisers

50
00:02:21,110 --> 00:02:24,280
that have budgets, a lot of times,
if you don't control for

51
00:02:24,280 --> 00:02:27,880
the budgets properly,
the effect can change as you ramp up.

52
00:02:27,880 --> 00:02:30,770
>> Okay, and how would I catch things
like this during the experiment,

53
00:02:30,770 --> 00:02:34,120
rather than after the launch has
happened and something went wrong?

54
00:02:34,120 --> 00:02:36,580
>> This is where you get into
the more complicated analyses and

55
00:02:36,580 --> 00:02:38,760
in the more complicated
experiment designs.

56
00:02:38,760 --> 00:02:42,120
We had talked in lesson four about
using pre- and post-periods.

57
00:02:42,120 --> 00:02:44,400
And so if you're really concerned
about a learning effect,

58
00:02:44,400 --> 00:02:46,210
you probably want to use those pre- and

59
00:02:46,210 --> 00:02:49,600
those post-periods, combined with
a cohort analysis, to try and

60
00:02:49,600 --> 00:02:52,910
understand how your users are basically
adapting to the change over time.
