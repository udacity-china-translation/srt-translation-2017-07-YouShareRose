1
00:00:00,390 --> 00:00:02,120
You mentioned that to
choose a summary metric,

2
00:00:02,120 --> 00:00:05,490
we need to think about the sensitivity
and the robustness of the metric.

3
00:00:05,490 --> 00:00:06,700
Can you explain that more?

4
00:00:06,700 --> 00:00:07,740
>> Yeah, so in lesson one,

5
00:00:07,740 --> 00:00:11,660
we talked about the sensitivity of
a test to a change that you care about.

6
00:00:11,660 --> 00:00:14,600
The sensitivity of a metric
is kind of the same thing.

7
00:00:14,600 --> 00:00:18,310
The idea is that you want to choose
a metric that picks up changes you care

8
00:00:18,310 --> 00:00:21,520
about but,
this is the idea of robustness,

9
00:00:21,520 --> 00:00:23,780
is robust against changes
that you don't care about.

10
00:00:23,780 --> 00:00:27,030
So it doesn't move a lot when
nothing interesting has happened.

11
00:00:27,030 --> 00:00:30,230
>> I think that makes sense, but
could you give me some examples?

12
00:00:30,230 --> 00:00:31,000
>> Sure.
So

13
00:00:31,000 --> 00:00:35,020
on our latency example where we're
looking at the load time of a video,

14
00:00:35,020 --> 00:00:38,040
that's a great example of
kind of classical statistics.

15
00:00:38,040 --> 00:00:40,090
Do you use the mean or
do you use the median?

16
00:00:40,090 --> 00:00:42,380
Or, in our case, you may use neither.

17
00:00:42,380 --> 00:00:44,980
And the idea is that the mean
is sensitive to outliers.

18
00:00:44,980 --> 00:00:48,985
So, if in your data you see a lot
of cases of really long load times,

19
00:00:48,985 --> 00:00:52,690
maybe due to something going on in
the users machine, or a bad network

20
00:00:52,690 --> 00:00:56,710
connection, then you want to maybe not
choose the mean, because the mean is

21
00:00:56,710 --> 00:01:00,340
going to be pretty heavily influenced
by those types of observations.

22
00:01:00,340 --> 00:01:03,000
And so that's called not being robust.

23
00:01:03,000 --> 00:01:05,285
Now, on the other hand,
you could choose the median,

24
00:01:05,285 --> 00:01:10,310
which tends to be much more robust
to that type of behavior, but if you

25
00:01:10,310 --> 00:01:14,190
only affect a fraction of your users,
even if it's a fairly large fraction,

26
00:01:14,190 --> 00:01:18,400
like 20% with a change, you might
not see the median move at all.

27
00:01:18,400 --> 00:01:19,810
So the median is robust.

28
00:01:19,810 --> 00:01:20,590
But in this case,

29
00:01:20,590 --> 00:01:23,330
you might want to actually consider
using some other statistics,

30
00:01:23,330 --> 00:01:28,180
such as the 90th or the 99th percentile,
and see how those change as well.

31
00:01:28,180 --> 00:01:31,480
So they might be a better reflection of
what you're actually trying to measure.

32
00:01:31,480 --> 00:01:31,990
>> Okay.

33
00:01:31,990 --> 00:01:35,110
So how would you actually measure
the sensitivity and robustness?

34
00:01:35,110 --> 00:01:38,580
>> Well, there are two main ways and
ideally, you could do both.

35
00:01:38,580 --> 00:01:40,790
The first has to do with
running experiments or

36
00:01:40,790 --> 00:01:43,010
using experiments you already have.

37
00:01:43,010 --> 00:01:47,850
So, for example, in our latency example,
we could run a few simple experiments

38
00:01:47,850 --> 00:01:50,920
where we say, increase the quality
of the video which should,

39
00:01:50,920 --> 00:01:53,990
in theory,
increase the load time for users.

40
00:01:53,990 --> 00:01:58,278
And we could see if the metrics were
interested in, actually respond to that.

41
00:01:58,278 --> 00:02:00,607
Now the date is going to
get a little fuzzier so

42
00:02:00,607 --> 00:02:04,468
they may not respond exactly that way
we think, but we should be able to tell

43
00:02:04,468 --> 00:02:07,993
if they're actually moving in a way
that intuitively makes sense.

44
00:02:07,993 --> 00:02:12,151
And we can also use what were called
a versus a experiments to determine if

45
00:02:12,151 --> 00:02:14,240
they're too sensitive.

46
00:02:14,240 --> 00:02:16,430
That's an experiment where
you don't change anything.

47
00:02:16,430 --> 00:02:19,690
You just compare people who saw
the same thing to each other.

48
00:02:19,690 --> 00:02:23,010
And you see if your metrics pick up any
spurious differences between the two.

49
00:02:23,010 --> 00:02:27,080
And that's a really critical element to
make sure that you're not going to be

50
00:02:27,080 --> 00:02:30,930
calling things significant that
maybe don't really mean anything.

51
00:02:30,930 --> 00:02:33,830
And then you can also look back
at experiments that were run by

52
00:02:33,830 --> 00:02:35,560
your company earlier.

53
00:02:35,560 --> 00:02:38,260
If you have experiments that
have happened before and

54
00:02:38,260 --> 00:02:40,280
you also now have
the benefit of hindsight,

55
00:02:40,280 --> 00:02:44,260
you may have user data about how well
people liked it and how they responded.

56
00:02:44,260 --> 00:02:45,220
You can certainly look and

57
00:02:45,220 --> 00:02:48,340
see if those experiments move
the metric you're interested in.

58
00:02:48,340 --> 00:02:52,390
Now, the second main category
of things you can look at is

59
00:02:52,390 --> 00:02:55,080
sort of a retrospective
analysis of your logs.

60
00:02:55,080 --> 00:02:58,720
If you don't have experiment data or
if you can't run new experiments,

61
00:02:58,720 --> 00:03:02,300
then you can look back at changes
you know you made to your site, and

62
00:03:02,300 --> 00:03:05,570
see if the metrics you're interested in
actually moved in conjunction with those

63
00:03:05,570 --> 00:03:06,560
changes.

64
00:03:06,560 --> 00:03:08,680
Or you can just look at
the history of the the metric and

65
00:03:08,680 --> 00:03:12,090
see if you can find a cause for
any major changes that you see.

66
00:03:12,090 --> 00:03:14,460
That can help you get some good
intuition about what's going on.
